Group Member UIDs:
Calvin Cam 803781760

Ian Chen 203764136

1a Implementation:
- Tokenizer by Calvin
- Parser by Ian


Extra Notes by Calvin:
- Originally I wrote a make_command_stream to read in the script and check for syntax errors before starting. 
  Once it generates a command_stream of the script, it will parse through this command and create the tree by
  finding the most precedent operator. The left and right side are nodes which will be recursively parsed.
  This will repeat till a stopping case. read_command_stream will pull from this created tree.
- However, my friend suggest a smarter method which involves tokenizing and then parsing. This method would
  remove all the error checking in the beginning so that you would only need to check a small command to fit
  under a certain command type. He suggested to have the following implementation:

      typedef struct TOKEN {
          enum token_type type;
          char *token_name;
      } token_t;
      
      int remove_whitespace(cstream_t name) {
          // Keep reading from name until a non-whitespace character is reached, then unget that character
      }
      
      token_t get_next_token(cstream_t name) {
          // First, consume the whitespace by calling remove_white
          // Next, read the next character. If it's for a special token, call a handle_reserved(name,letter)
          //    function which reads until the next non-special letter.
          // Otherwise just read characters until you reach a special letter or whitespace and create a token
          //     from those characters.
      }

- My tokenizer was centered around:
  + http://stackoverflow.com/questions/1669/learning-to-write-a-compiler
  + http://en.wikipedia.org/wiki/Lexing
  + My friend's psuedocode shown above

Notes by Ian:
- The second part of project 1a involves taking token objects from the tokenizer and converting them into commands to
  be stored in a tree. read_command_stream continues to take in tokens and store them in a dynamically allocated token 
  array. We then call parse on this token array. 
- The parse function reads in the token array and breaks it down by recursively calling parse on subsequently smaller 
  pieces of the array. In this manner it is a little similar to mergesort. Depending on the type of command read from 
  tokens, fields of the struct command are set and linked with other commands.
- The parse function is not very efficient and may need some improvement with parts 1b and 1c for more effective 
  parallelization


1b Implementation:
- We executed commands through an int function execute which takes in a command_t.
- This function is called when time travel is false.
- The function returns 1 when the command executes successfully and 0 if it failed.
- Each command is implemented based on shell's definition.
- Sequence runs the first command, then the second.
- Pipe is more difficult to keep track of due to the multiple processes. To be safe, we killed processes and used wait 
  to keep track of status.
- And runs both commands.
- Or runs one command and exits if it is successful else it would run the second.
- Subshell requires opening files as shown in class.
- Simple is similar to subshell except the "exec" command was interesting to implement.
- Overall, 1b was good practice on forks and dups and processes in general. Hopefully we implemented everything required
  for command execution.


1c Implementation by Ian:
- We implement by creating a graph struct containing pointers to two graph arrays. These arrays are for 
  dependencies and executables. After processing the commands from command_stream passed in main, the dependency graph
  is set up and executed.
- Timetravel param is passed into main and from there, determines which execution function to call (either basic execute or 
  special parallelization execute)
- 
